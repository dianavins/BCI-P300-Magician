import pandas as pd
import numpy as np
import os

from keras.layers import *
from keras.models import Model
from keras import Sequential
from keras.callbacks import ModelCheckpoint
from keras.callbacks import EarlyStopping
from keras.models import load_model

def read_csv_files(folder_path):    
    """
    Reads all of the csv files into an array
    
    Args:
        - folder_path: str of the path of the folder
    
    Returns:
        - all_df: list of all dataframes
    """
    dfs = []
    for filename in os.listdir(folder_path):
        if filename.endswith('.csv'):
            file_path = os.path.join(folder_path, filename)
            # Read the CSV file into a DataFrame, ignoring lines starting with '#'
            df = pd.read_csv(file_path, comment='#')
            dfs.append(df)
    return dfs

def get_training_data(dfs, x_cols, y_col):
    """
    Converts the data from the dataframe into np arrays for the
    model
    
    Args:
        - df: list of dataframes to pull the data from
        - x_cols: list of strings, column titles for the training data
        - y_col: string column title for the label of the data
        - test: boolean default false, don't worry about this
    
    Returns:
        - inputs: np array (7x78xlen(df-78))
        - outputs: np array (1X78xlen(df-78))
    """
    inputs = list()
    outputs = list()
    for df in dfs:
        x = df[x_cols].to_numpy()
        y = df[[y_col]].to_numpy()
        
        # creates arrays of size 78x7
        inputs += [np.array(x[i : i+78]) for i in range(len(x) - 78)]
        outputs += [np.array(y[i : i+78]) for i in range(len(x) - 78)]
    
    return np.array(inputs), np.array(outputs)


# Bellow this is the model (try to change training parameters before this)

# Randomizing function for bias and weights of the network
def cecotti_normal(shape, dtype = None, partition_info = None):
    if len(shape) == 1:
        fan_in = shape[0]
    elif len(shape) == 2:
        fan_in = shape[0]
    else:
        receptive_field_size = 1
        for dim in shape[:-2]:
            receptive_field_size *= dim
        fan_in = shape[-2] * receptive_field_size
    return K.random_normal(shape, mean = 0.0, stddev = (1.0 / fan_in))

# Custom tanh activation function
def scaled_tanh(z):
    return 1.7159 * K.tanh((2.0 / 3.0) * z)

#actual model itself
def CNN2a_model(channels=8, filters=10):
    model = Sequential([
        Conv1D(
            filters = filters,
            kernel_size = 1,
            padding = "same",
            bias_initializer = cecotti_normal,
            kernel_initializer = cecotti_normal,
            use_bias = True,
            activation = scaled_tanh,
            input_shape = (78, channels)
        ),
        Conv1D(
            filters = 50,
            kernel_size = 13,
            padding = "valid",
            strides = 11,
            bias_initializer = cecotti_normal,
            kernel_initializer = cecotti_normal,
            use_bias = True,
            activation = scaled_tanh,
        ),
        Flatten(),
        Dense(100, activation="sigmoid"),
        Dense(1, activation="sigmoid")
    ])
    model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])
    return model

def train(folder_path, train_cols, save_path = 'models/model.h5', BATCH_SIZE = 256, EPOCHS = 50, VALID_SPLIT = 0.05, SHUFFLE = 1, transfer = False, model_path = ''):
    """
    Trains the model on the data given in the folder and saves the data into the specified
    save path. Can fine tune parameters to inprove accuracy
    
    Args
        - folder_path: string; path of the folder containg all the training data
        - train_cols: list of strings; columns in csvs in folders to train off of
        - save_path: string; default models/model.h5; path to save the best model to
        - BATCH_SIZE: int; default 256; size of each training batch
        - EPOCHS: int; default 50; how many times to train
        - VALID SPLIT: float; default 0.5; proportion of data to be used as validation data
        - SHUFFLE: int; default 1; 1 = shuffle sub groups 0 = don't
        - transfer: boolean; default False; experimental rn but will be for transfer learning
        - model_path: str; experimental rn but will be filepath of model to do transfer learning
    """
    
    # Loads data
    df = read_csv_files(folder_path)
    train_features, train_labels = get_training_data(df, train_cols, 'Trigger')
    
    
    # Model definition
    if not transfer:
        model = CNN2a_model(channels=7, filters=10)
    else:
        model = load_model(model_path)

    # Callback to save best model only
    checkpoint = ModelCheckpoint(filepath= save_path,
                                 monitor='val_loss',
                                 mode='min',
                                 save_best_only=True)

    # Callback to stop when loss on validation set doesn't decrease in 50 epochs
    earlystop = EarlyStopping(monitor = 'val_loss',
                                  mode = 'min',
                                  patience = 50,
                                  restore_best_weights = True)

    # Callback to keep track of model statistics
    history = model.fit(x=train_features,
                        y=train_labels,
                        batch_size=BATCH_SIZE,
                        epochs=EPOCHS,
                        validation_split=VALID_SPLIT,
                        callbacks=[checkpoint, earlystop],
                        shuffle=SHUFFLE)

train_cols = ['LE', 'F4', 'C4', 'P4', 'P3', 'C3', 'F3']
train('data', train_cols)

